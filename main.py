import os
import json
import arxiv
import requests
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Tuple
import time
import numpy as np
from collections import Counter, defaultdict
import re

# C·∫•u h√¨nh th∆∞ m·ª•c
BASE_DIR = Path(__file__).parent
DATA_DIR = BASE_DIR / "data"
REPORTS_DIR = BASE_DIR / "reports"
MODELS_DIR = BASE_DIR / "models"
DATA_DIR.mkdir(exist_ok=True)
REPORTS_DIR.mkdir(exist_ok=True)
MODELS_DIR.mkdir(exist_ok=True)


class DataCollectorAgent:
    """Agent thu th·∫≠p d·ªØ li·ªáu t·ª´ ArXiv v√† c√°c ngu·ªìn mi·ªÖn ph√≠ v·ªõi limit linh ho·∫°t"""
    
    def __init__(self):
        self.arxiv_client = arxiv.Client()
    
    def fetch_arxiv_papers(self, query: str, limit: int = 50, days_back: int = 30) -> List[Dict]:
        """Thu th·∫≠p papers t·ª´ ArXiv v·ªõi limit t√πy ch·ªânh"""
        print(f"üîç ƒêang t√¨m ki·∫øm papers v·ªÅ '{query}' tr√™n ArXiv (limit: {limit})...")
        
        search = arxiv.Search(
            query=query,
            max_results=limit,
            sort_by=arxiv.SortCriterion.SubmittedDate
        )
        
        papers = []
        cutoff_date = datetime.now() - timedelta(days=days_back)
        
        for result in self.arxiv_client.results(search):
            if result.published.replace(tzinfo=None) < cutoff_date:
                continue
                
            paper_data = {
                'title': result.title,
                'authors': [author.name for author in result.authors],
                'abstract': result.summary,
                'published': result.published.isoformat(),
                'pdf_url': result.pdf_url,
                'categories': result.categories,
                'source': 'arxiv'
            }
            papers.append(paper_data)
        
        print(f"‚úÖ ƒê√£ thu th·∫≠p {len(papers)} papers t·ª´ ArXiv")
        return papers
    
    def fetch_semantic_scholar(self, query: str, limit: int = 50) -> List[Dict]:
        """Thu th·∫≠p papers t·ª´ Semantic Scholar API v·ªõi limit t√πy ch·ªânh"""
        print(f"üîç ƒêang t√¨m ki·∫øm papers tr√™n Semantic Scholar (limit: {limit})...")
        
        url = "https://api.semanticscholar.org/graph/v1/paper/search"
        params = {
            'query': query,
            'limit': min(limit, 100),  # API limit
            'fields': 'title,abstract,authors,year,publicationDate,citationCount,url,venue'
        }
        
        try:
            response = requests.get(url, params=params, timeout=10)
            if response.status_code == 200:
                data = response.json()
                papers = []
                
                for paper in data.get('data', []):
                    if paper.get('abstract'):
                        paper_data = {
                            'title': paper.get('title', ''),
                            'authors': [a.get('name', '') for a in paper.get('authors', [])],
                            'abstract': paper.get('abstract', ''),
                            'published': paper.get('publicationDate', ''),
                            'citations': paper.get('citationCount', 0),
                            'url': paper.get('url', ''),
                            'venue': paper.get('venue', ''),
                            'source': 'semantic_scholar'
                        }
                        papers.append(paper_data)
                
                print(f"‚úÖ ƒê√£ thu th·∫≠p {len(papers)} papers t·ª´ Semantic Scholar")
                return papers
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi truy c·∫≠p Semantic Scholar: {e}")
        
        return []
    
    def fetch_all_papers(self, query: str, limit_per_source: int = 50, days_back: int = 30) -> List[Dict]:
        """Thu th·∫≠p papers t·ª´ t·∫•t c·∫£ ngu·ªìn v·ªõi limit th·ªëng nh·∫•t"""
        print(f"\nüìö Thu th·∫≠p d·ªØ li·ªáu v·ªõi limit: {limit_per_source} papers/ngu·ªìn")
        print("=" * 60)
        
        arxiv_papers = self.fetch_arxiv_papers(query, limit_per_source, days_back)
        time.sleep(2)  # Rate limiting
        semantic_papers = self.fetch_semantic_scholar(query, limit_per_source)
        
        all_papers = arxiv_papers + semantic_papers
        print(f"\nüìä T·ªïng c·ªông: {len(all_papers)} papers t·ª´ {2} ngu·ªìn")
        return all_papers
    
    def save_papers(self, papers: List[Dict], filename: str = "papers.json"):
        """L∆∞u papers v√†o file JSON"""
        filepath = DATA_DIR / filename
        
        # Load existing papers
        existing_papers = []
        if filepath.exists():
            with open(filepath, 'r', encoding='utf-8') as f:
                existing_papers = json.load(f)
        
        # Merge and remove duplicates
        all_papers = existing_papers + papers
        unique_papers = {p['title']: p for p in all_papers}.values()
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(list(unique_papers), f, indent=2, ensure_ascii=False)
        
        print(f"üíæ ƒê√£ l∆∞u {len(unique_papers)} papers v√†o {filepath}")
        return filepath


class AdvancedSummarizerAgent:
    """Agent t√≥m t·∫Øt papers v·ªõi thu·∫≠t to√°n TextRank v√† TF-IDF"""
    
    def __init__(self):
        self.stopwords = self._load_stopwords()
    
    def _load_stopwords(self) -> set:
        """Load English stopwords"""
        return set([
            'a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any',
            'are', 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between',
            'both', 'but', 'by', 'can', 'did', 'do', 'does', 'doing', 'down', 'during', 'each',
            'few', 'for', 'from', 'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',
            'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'it',
            'its', 'itself', 'just', 'me', 'might', 'more', 'most', 'must', 'my', 'myself', 'no',
            'nor', 'not', 'now', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours',
            'ourselves', 'out', 'over', 'own', 'same', 'she', 'should', 'so', 'some', 'such', 'than',
            'that', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they',
            'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', 'we',
            'were', 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with',
            'would', 'you', 'your', 'yours', 'yourself', 'yourselves', 'also', 'may', 'however',
            'within', 'show', 'using', 'used', 'use', 'based', 'can', 'paper', 'propose', 'proposed',
            'show', 'shows', 'study', 'studies', 'present', 'presented', 'approach', 'approaches'
        ])
    
    def _preprocess_text(self, text: str) -> List[str]:
        """Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n"""
        sentences = re.split(r'[.!?]+', text.lower())
        sentences = [s.strip() for s in sentences if len(s.strip()) > 20]
        return sentences
    
    def _calculate_sentence_similarity(self, sent1: str, sent2: str) -> float:
        """T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa 2 c√¢u d·ª±a tr√™n Jaccard similarity"""
        words1 = set(re.findall(r'\b\w+\b', sent1)) - self.stopwords
        words2 = set(re.findall(r'\b\w+\b', sent2)) - self.stopwords
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0
    
    def _build_similarity_matrix(self, sentences: List[str]) -> np.ndarray:
        """X√¢y d·ª±ng ma tr·∫≠n t∆∞∆°ng ƒë·ªìng gi·ªØa c√°c c√¢u"""
        n = len(sentences)
        similarity_matrix = np.zeros((n, n))
        
        for i in range(n):
            for j in range(n):
                if i != j:
                    similarity_matrix[i][j] = self._calculate_sentence_similarity(
                        sentences[i], sentences[j]
                    )
        
        return similarity_matrix
    
    def _pagerank(self, similarity_matrix: np.ndarray, damping: float = 0.85, 
                  max_iter: int = 100, tol: float = 1e-6) -> np.ndarray:
        """Thu·∫≠t to√°n PageRank ƒë·ªÉ x·∫øp h·∫°ng c√¢u"""
        n = similarity_matrix.shape[0]
        
        # Normalize matrix
        row_sums = similarity_matrix.sum(axis=1)
        normalized_matrix = similarity_matrix / (row_sums[:, np.newaxis] + 1e-10)
        
        # Initialize ranks
        ranks = np.ones(n) / n
        
        # Iterate
        for _ in range(max_iter):
            prev_ranks = ranks.copy()
            ranks = (1 - damping) / n + damping * normalized_matrix.T.dot(ranks)
            
            if np.linalg.norm(ranks - prev_ranks) < tol:
                break
        
        return ranks
    
    def textrank_summarize(self, text: str, num_sentences: int = 3) -> str:
        """T√≥m t·∫Øt vƒÉn b·∫£n b·∫±ng thu·∫≠t to√°n TextRank"""
        sentences = self._preprocess_text(text)
        
        if len(sentences) <= num_sentences:
            return '. '.join(sentences) + '.'
        
        similarity_matrix = self._build_similarity_matrix(sentences)
        ranks = self._pagerank(similarity_matrix)
        
        top_indices = ranks.argsort()[-num_sentences:][::-1]
        top_indices = sorted(top_indices)
        
        summary = '. '.join([sentences[i] for i in top_indices]) + '.'
        return summary.capitalize()
    
    def tfidf_summarize(self, text: str, num_sentences: int = 3) -> str:
        """T√≥m t·∫Øt vƒÉn b·∫£n b·∫±ng TF-IDF"""
        sentences = self._preprocess_text(text)
        
        if len(sentences) <= num_sentences:
            return '. '.join(sentences) + '.'
        
        word_freq = Counter()
        for sentence in sentences:
            words = [w for w in re.findall(r'\b\w+\b', sentence) if w not in self.stopwords]
            word_freq.update(words)
        
        doc_freq = Counter()
        for sentence in sentences:
            words = set([w for w in re.findall(r'\b\w+\b', sentence) if w not in self.stopwords])
            doc_freq.update(words)
        
        n_docs = len(sentences)
        idf = {word: np.log(n_docs / (freq + 1)) for word, freq in doc_freq.items()}
        
        sentence_scores = []
        for sentence in sentences:
            words = [w for w in re.findall(r'\b\w+\b', sentence) if w not in self.stopwords]
            if not words:
                sentence_scores.append(0)
                continue
            
            score = sum(word_freq[w] * idf.get(w, 0) for w in words) / len(words)
            sentence_scores.append(score)
        
        top_indices = np.argsort(sentence_scores)[-num_sentences:][::-1]
        top_indices = sorted(top_indices)
        
        summary = '. '.join([sentences[i] for i in top_indices]) + '.'
        return summary.capitalize()
    
    def summarize_papers(self, papers: List[Dict], method: str = 'textrank') -> List[Dict]:
        """T√≥m t·∫Øt t·∫•t c·∫£ papers"""
        print(f"üìù ƒêang t√≥m t·∫Øt {len(papers)} papers b·∫±ng ph∆∞∆°ng ph√°p {method.upper()}...")
        
        for paper in papers:
            if 'summary' not in paper:
                try:
                    if method == 'textrank':
                        paper['summary'] = self.textrank_summarize(paper['abstract'], num_sentences=3)
                    elif method == 'tfidf':
                        paper['summary'] = self.tfidf_summarize(paper['abstract'], num_sentences=3)
                    else:
                        paper['summary'] = paper['abstract'][:300] + '...'
                except Exception as e:
                    print(f"‚ö†Ô∏è L·ªói khi t√≥m t·∫Øt: {e}")
                    paper['summary'] = paper['abstract'][:300] + '...'
        
        print("‚úÖ Ho√†n th√†nh t√≥m t·∫Øt")
        return papers


class MLTrendAnalysisAgent:
    """Agent ph√¢n t√≠ch xu h∆∞·ªõng v·ªõi Machine Learning"""
    
    def __init__(self):
        self.stopwords = self._load_stopwords()
    
    def _load_stopwords(self) -> set:
        """Load stopwords"""
        return set([
            'a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any',
            'are', 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between',
            'both', 'but', 'by', 'can', 'did', 'do', 'does', 'doing', 'down', 'during', 'each',
            'few', 'for', 'from', 'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',
            'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'it',
            'its', 'itself', 'just', 'me', 'might', 'more', 'most', 'must', 'my', 'myself', 'no',
            'nor', 'not', 'now', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours',
            'ourselves', 'out', 'over', 'own', 'same', 'she', 'should', 'so', 'some', 'such', 'than',
            'that', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they',
            'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', 'we',
            'were', 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with',
            'would', 'you', 'your', 'yours', 'yourself', 'yourselves', 'paper', 'study', 'research'
        ])
    
    def extract_ngrams(self, text: str, n: int = 2, top_k: int = 50) -> List[Tuple[str, int]]:
        """Tr√≠ch xu·∫•t n-grams v·ªõi TF-IDF weighting"""
        words = re.findall(r'\b[a-z]{3,}\b', text.lower())
        words = [w for w in words if w not in self.stopwords]
        
        ngrams = []
        for i in range(len(words) - n + 1):
            ngram = ' '.join(words[i:i+n])
            ngrams.append(ngram)
        
        ngram_freq = Counter(ngrams)
        return ngram_freq.most_common(top_k)
    
    def cluster_topics_kmeans(self, papers: List[Dict], n_clusters: int = 5) -> Dict:
        """Clustering topics b·∫±ng K-Means v·ªõi TF-IDF vectors"""
        print("ü§ñ ƒêang ph√¢n c·ª•m topics b·∫±ng K-Means...")
        
        texts = [p['title'] + ' ' + p['abstract'] for p in papers]
        
        vocabulary = Counter()
        for text in texts:
            words = [w for w in re.findall(r'\b[a-z]{3,}\b', text.lower()) 
                    if w not in self.stopwords]
            vocabulary.update(words)
        
        top_words = [word for word, _ in vocabulary.most_common(500)]
        word_to_idx = {word: idx for idx, word in enumerate(top_words)}
        
        vectors = []
        for text in texts:
            words = [w for w in re.findall(r'\b[a-z]{3,}\b', text.lower()) 
                    if w not in self.stopwords and w in word_to_idx]
            
            word_freq = Counter(words)
            vector = np.zeros(len(top_words))
            
            for word, freq in word_freq.items():
                idx = word_to_idx[word]
                vector[idx] = freq
            
            vectors.append(vector)
        
        X = np.array(vectors)
        X = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-10)
        
        n_clusters = min(n_clusters, len(papers))
        centroids = X[np.random.choice(len(X), n_clusters, replace=False)]
        
        for _ in range(50):
            distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)
            labels = np.argmin(distances, axis=1)
            
            new_centroids = np.array([X[labels == i].mean(axis=0) if np.any(labels == i) 
                                     else centroids[i] for i in range(n_clusters)])
            
            if np.allclose(centroids, new_centroids):
                break
            
            centroids = new_centroids
        
        clusters = defaultdict(list)
        cluster_keywords = {}
        
        for idx, label in enumerate(labels):
            clusters[label].append(papers[idx])
        
        for cluster_id, cluster_papers in clusters.items():
            cluster_text = ' '.join([p['title'] + ' ' + p['abstract'] 
                                    for p in cluster_papers])
            
            words = [w for w in re.findall(r'\b[a-z]{3,}\b', cluster_text.lower()) 
                    if w not in self.stopwords]
            
            word_freq = Counter(words)
            top_keywords = [word for word, _ in word_freq.most_common(10)]
            
            cluster_keywords[f"Cluster {cluster_id}"] = {
                'keywords': top_keywords,
                'size': len(cluster_papers),
                'papers': cluster_papers[:3]
            }
        
        print(f"‚úÖ ƒê√£ ph√¢n th√†nh {len(cluster_keywords)} clusters")
        return cluster_keywords
    
    def identify_trends(self, papers: List[Dict]) -> Dict:
        """X√°c ƒë·ªãnh xu h∆∞·ªõng v·ªõi ML clustering"""
        print("üìä ƒêang ph√¢n t√≠ch xu h∆∞·ªõng v·ªõi Machine Learning...")
        
        all_text = ' '.join([p['title'] + ' ' + p['abstract'] for p in papers])
        
        unigrams = self.extract_ngrams(all_text, n=1, top_k=30)
        keywords = dict(unigrams)
        
        bigrams = self.extract_ngrams(all_text, n=2, top_k=30)
        phrases = dict(bigrams)
        
        trigrams = self.extract_ngrams(all_text, n=3, top_k=20)
        
        clusters = self.cluster_topics_kmeans(papers, n_clusters=7)
        
        temporal_trends = self._analyze_temporal_trends(papers)
        
        print("‚úÖ Ho√†n th√†nh ph√¢n t√≠ch xu h∆∞·ªõng")
        
        return {
            'keywords': keywords,
            'phrases': phrases,
            'trigrams': dict(trigrams),
            'clusters': clusters,
            'temporal_trends': temporal_trends,
            'total_papers': len(papers)
        }
    
    def _analyze_temporal_trends(self, papers: List[Dict]) -> Dict:
        """Ph√¢n t√≠ch xu h∆∞·ªõng theo th·ªùi gian"""
        temporal_data = defaultdict(int)
        
        for paper in papers:
            pub_date = paper.get('published', '')
            if pub_date:
                month = pub_date[:7]
                temporal_data[month] += 1
        
        return dict(sorted(temporal_data.items()))


class AdvancedIdeaGeneratorAgent:
    """Agent t·∫°o √Ω t∆∞·ªüng nghi√™n c·ª©u th√¥ng minh v·ªõi scoring system"""
    
    def __init__(self):
        self.idea_templates = [
            "Ph√°t tri·ªÉn {method} cho {application} s·ª≠ d·ª•ng {technique}",
            "Nghi√™n c·ª©u t√≠ch h·ª£p {concept1} v√† {concept2} ƒë·ªÉ {goal}",
            "ƒê√°nh gi√° hi·ªáu qu·∫£ c·ªßa {approach} trong {domain}",
            "T·ªëi ∆∞u h√≥a {system} th√¥ng qua {optimization}",
            "Kh·∫£o s√°t to√†n di·ªán v·ªÅ {topic} v√† ·ª©ng d·ª•ng trong {field}",
            "X√¢y d·ª±ng framework {name} cho {purpose}",
            "Ph√¢n t√≠ch so s√°nh gi·ªØa {method1} v√† {method2} trong {context}",
            "C·∫£i thi·ªán {aspect} c·ªßa {system} b·∫±ng {technique}",
        ]
    
    def _score_idea(self, keywords: List[str], trends: Dict) -> float:
        """T√≠nh ƒëi·ªÉm cho √Ω t∆∞·ªüng d·ª±a tr√™n xu h∆∞·ªõng"""
        score = 0.0
        
        for keyword in keywords:
            if keyword in trends['keywords']:
                score += trends['keywords'][keyword]
            if keyword in trends['phrases']:
                score += trends['phrases'][keyword] * 1.5
        
        return score
    
    def _find_research_gaps(self, clusters: Dict) -> List[Dict]:
        """T√¨m c√°c gap trong nghi√™n c·ª©u"""
        gaps = []
        
        cluster_sizes = [(name, info['size']) for name, info in clusters.items()]
        cluster_sizes.sort(key=lambda x: x[1])
        
        for cluster_name, size in cluster_sizes[:3]:
            if size < 5:
                gap = {
                    'area': cluster_name,
                    'papers': size,
                    'keywords': clusters[cluster_name]['keywords'][:5],
                    'type': 'underexplored'
                }
                gaps.append(gap)
        
        return gaps
    
    def _generate_combination_ideas(self, clusters: Dict) -> List[Dict]:
        """T·∫°o √Ω t∆∞·ªüng t·ª´ vi·ªác k·∫øt h·ª£p c√°c clusters"""
        ideas = []
        cluster_list = list(clusters.items())
        
        for i in range(min(3, len(cluster_list))):
            for j in range(i + 1, min(5, len(cluster_list))):
                cluster1_name, cluster1_info = cluster_list[i]
                cluster2_name, cluster2_info = cluster_list[j]
                
                keywords1 = cluster1_info['keywords'][:3]
                keywords2 = cluster2_info['keywords'][:3]
                
                idea = {
                    'title': f"T√≠ch h·ª£p {keywords1[0]} v√† {keywords2[0]} trong h·ªá th·ªëng AI",
                    'description': f"K·∫øt h·ª£p insights t·ª´ {cluster1_name} v√† {cluster2_name}",
                    'keywords': keywords1 + keywords2,
                    'clusters': [cluster1_name, cluster2_name],
                    'type': 'combination'
                }
                ideas.append(idea)
        
        return ideas
    
    def generate_research_ideas(self, trends: Dict, papers: List[Dict]) -> List[Dict]:
        """T·∫°o √Ω t∆∞·ªüng nghi√™n c·ª©u th√¥ng minh"""
        print("üí° ƒêang t·∫°o √Ω t∆∞·ªüng nghi√™n c·ª©u v·ªõi AI...")
        
        ideas = []
        
        # 1. √ù t∆∞·ªüng t·ª´ trending keywords
        top_keywords = list(trends['keywords'].keys())[:10]
        top_phrases = list(trends['phrases'].keys())[:10]
        
        for i in range(min(5, len(top_phrases))):
            phrase = top_phrases[i]
            keyword = top_keywords[i] if i < len(top_keywords) else top_phrases[i]
            
            idea = {
                'title': self.idea_templates[i % len(self.idea_templates)].format(
                    method=phrase,
                    application=keyword,
                    technique=top_keywords[(i+1) % len(top_keywords)],
                    concept1=phrase,
                    concept2=top_phrases[(i+1) % len(top_phrases)],
                    goal="c·∫£i thi·ªán hi·ªáu su·∫•t v√† ƒë·ªô ch√≠nh x√°c",
                    approach=phrase,
                    domain="Agentic AI systems",
                    system=keyword,
                    optimization=phrase,
                    topic=phrase,
                    field=keyword,
                    name=phrase.title(),
                    purpose=keyword,
                    method1=phrase,
                    method2=top_phrases[(i+1) % len(top_phrases)],
                    context=keyword,
                    aspect="performance"
                ),
                'description': f"D·ª±a tr√™n {trends['keywords'].get(keyword, 0)} mentions",
                'keywords': [phrase, keyword],
                'score': self._score_idea([phrase, keyword], trends),
                'priority': 'High' if i < 2 else 'Medium',
                'type': 'trending'
            }
            ideas.append(idea)
        
        # 2. √ù t∆∞·ªüng t·ª´ cluster analysis
        if 'clusters' in trends:
            combination_ideas = self._generate_combination_ideas(trends['clusters'])
            ideas.extend(combination_ideas[:3])
            
            # 3. √ù t∆∞·ªüng t·ª´ research gaps
            gaps = self._find_research_gaps(trends['clusters'])
            for gap in gaps:
                idea = {
                    'title': f"Kh√°m ph√° {gap['area']}: H∆∞·ªõng nghi√™n c·ª©u m·ªõi",
                    'description': f"Ch·ªâ c√≥ {gap['papers']} papers, c·∫ßn nghi√™n c·ª©u s√¢u h∆°n",
                    'keywords': gap['keywords'],
                    'priority': 'High',
                    'type': 'gap_filling',
                    'score': 100 / (gap['papers'] + 1)
                }
                ideas.append(idea)
        
        # 4. √ù t∆∞·ªüng t·ª´ temporal trends
        if 'temporal_trends' in trends:
            recent_months = list(trends['temporal_trends'].keys())[-3:]
            if recent_months:
                idea = {
                    'title': "Ph√¢n t√≠ch xu h∆∞·ªõng m·ªõi nh·∫•t trong Agentic AI",
                    'description': f"T·∫≠p trung v√†o c√°c nghi√™n c·ª©u t·ª´ {recent_months[0]} ƒë·∫øn nay",
                    'keywords': top_keywords[:5],
                    'priority': 'High',
                    'type': 'temporal',
                    'score': sum(trends['temporal_trends'][m] for m in recent_months)
                }
                ideas.append(idea)
        
        # Sort by score
        ideas.sort(key=lambda x: x.get('score', 0), reverse=True)
        
        print(f"‚úÖ ƒê√£ t·∫°o {len(ideas)} √Ω t∆∞·ªüng nghi√™n c·ª©u")
        return ideas[:15]  # Top 15 ideas


class ReportGeneratorAgent:
    """Agent t·∫°o b√°o c√°o markdown"""

    def create_markdown_report(self, papers: List[Dict], trends: Dict, 
                               ideas: List[Dict], output_file: str = None) -> str:
        """T·∫°o b√°o c√°o markdown chi ti·∫øt"""
        print("üßæ ƒêang t·∫°o b√°o c√°o...")

        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"research_report_{timestamp}.md"

        filepath = REPORTS_DIR / output_file

        with open(filepath, 'w', encoding='utf-8') as f:
            # Header
            f.write("# üìò B√°o C√°o Nghi√™n C·ª©u Agentic AI\n\n")
            f.write(f"**Ng√†y t·∫°o:** {datetime.now().strftime('%d/%m/%Y %H:%M')}\n\n")
            f.write(f"**T·ªïng s·ªë papers:** {len(papers)}\n\n")
            f.write("---\n\n")

            # Executive Summary
            f.write("## üß† T√≥m T·∫Øt ƒêi·ªÅu H√†nh\n\n")
            f.write(f"B√°o c√°o n√†y t·ªïng h·ª£p {len(papers)} b√†i b√°o nghi√™n c·ª©u m·ªõi nh·∫•t ")
            f.write("v·ªÅ Agentic AI, ph√¢n t√≠ch xu h∆∞·ªõng c√¥ng ngh·ªá v√† ƒë·ªÅ xu·∫•t ")
            f.write("c√°c h∆∞·ªõng nghi√™n c·ª©u ti·ªÅm nƒÉng trong lƒ©nh v·ª±c n√†y.\n\n")

            # Trend Analysis
            f.write("## üìà Ph√¢n T√≠ch Xu H∆∞·ªõng\n\n")

            # Keywords
            if 'keywords' in trends:
                f.write("### üîë T·ª´ Kh√≥a N·ªïi B·∫≠t\n\n")
                f.write("| T·ª´ Kh√≥a | T·∫ßn Su·∫•t |\n")
                f.write("|----------|-----------|\n")
                for kw, freq in list(trends['keywords'].items())[:10]:
                    f.write(f"| {kw} | {freq} |\n")
                f.write("\n")

            # Phrases
            if 'phrases' in trends:
                f.write("### üß© C·ª•m T·ª´ Quan Tr·ªçng\n\n")
                f.write("| C·ª•m T·ª´ | T·∫ßn Su·∫•t |\n")
                f.write("|----------|-----------|\n")
                for phrase, freq in list(trends['phrases'].items())[:10]:
                    f.write(f"| {phrase} | {freq} |\n")
                f.write("\n")

            # Cluster Analysis
            if 'clusters' in trends:
                f.write("### ü™Ñ Ph√¢n C·ª•m Ch·ªß ƒê·ªÅ (Clusters)\n\n")
                for cluster_name, info in trends['clusters'].items():
                    f.write(f"#### üìö {cluster_name}\n\n")
                    f.write(f"- **S·ªë l∆∞·ª£ng papers:** {info['size']}\n")
                    f.write(f"- **T·ª´ kh√≥a n·ªïi b·∫≠t:** {', '.join(info['keywords'][:10])}\n\n")

            # Temporal Trends
            if 'temporal_trends' in trends:
                f.write("### ‚è≥ Xu H∆∞·ªõng Theo Th·ªùi Gian\n\n")
                for month, count in trends['temporal_trends'].items():
                    bar = "‚ñà" * min(count, 20)
                    f.write(f"- {month}: {count} papers {bar}\n")
                f.write("\n")

            # Research Ideas
            f.write("## üí° ƒê·ªÅ Xu·∫•t √ù T∆∞·ªüng Nghi√™n C·ª©u\n\n")
            for i, idea in enumerate(ideas, 1):
                f.write(f"### {i}. {idea['title']}\n\n")
                f.write(f"**M√¥ t·∫£:** {idea.get('description', 'Kh√¥ng c√≥ m√¥ t·∫£.')}\n\n")
                f.write(f"**ƒê·ªô ∆∞u ti√™n:** {idea.get('priority', 'N/A')}\n\n")
                f.write(f"**Lo·∫°i:** {idea.get('type', 'N/A')}\n\n")
                f.write(f"**T·ª´ kh√≥a li√™n quan:** {', '.join(idea.get('keywords', []))}\n\n")
                f.write(f"**ƒêi·ªÉm ƒë√°nh gi√°:** {idea.get('score', 0):.2f}\n\n")

            # Recent Papers
            f.write("## üì∞ C√°c B√†i B√°o M·ªõi Nh·∫•t\n\n")
            sorted_papers = sorted(
                papers, key=lambda x: x.get('published', ''), reverse=True
            )

            for i, paper in enumerate(sorted_papers[:20], 1):
                f.write(f"### {i}. {paper['title']}\n\n")
                authors = ', '.join(paper.get('authors', [])[:3])
                if len(paper.get('authors', [])) > 3:
                    authors += " et al."
                f.write(f"**T√°c gi·∫£:** {authors}\n\n")
                f.write(f"**Ng√†y xu·∫•t b·∫£n:** {paper.get('published', 'N/A')[:10]}\n\n")
                f.write(f"**Ngu·ªìn:** {paper.get('source', 'N/A')}\n\n")
                f.write(f"**T√≥m t·∫Øt:** {paper.get('summary', paper.get('abstract', '')[:300])}...\n\n")
                if 'pdf_url' in paper:
                    f.write(f"[T·∫£i PDF]({paper['pdf_url']})\n\n")
                elif 'url' in paper:
                    f.write(f"[ƒê·ªçc b√†i b√°o]({paper['url']})\n\n")
                f.write("---\n\n")

        print(f"‚úÖ B√°o c√°o ƒë√£ ƒë∆∞·ª£c t·∫°o t·∫°i: {filepath}")
        return str(filepath)



class AgenticResearchOrchestrator:
    """Orchestrator ƒëi·ªÅu ph·ªëi to√†n b·ªô quy tr√¨nh"""
    
    def __init__(self):
        self.collector = DataCollectorAgent()
        self.summarizer = AdvancedSummarizerAgent() 
        self.trend_analyzer = MLTrendAnalysisAgent()
        self.idea_generator = AdvancedIdeaGeneratorAgent() 
        self.report_generator = ReportGeneratorAgent()
    
    def run_research_pipeline(self, query: str = "Agentic AI", 
                             days_back: int = 30,
                             max_papers: int = 50):
        """Ch·∫°y to√†n b·ªô pipeline nghi√™n c·ª©u"""
        print("=" * 60)
        print(" B·∫ÆT ƒê·∫¶U AGENTIC RESEARCH PIPELINE")
        print("=" * 60)
        print()
        
        # Step 1: Thu th·∫≠p d·ªØ li·ªáu
        print("B∆Ø·ªöC 1: THU TH·∫¨P D·ªÆ LI·ªÜU")
        print("-" * 60)
        arxiv_papers = self.collector.fetch_arxiv_papers(query, max_papers, days_back)
        time.sleep(2)  # Rate limiting
        semantic_papers = self.collector.fetch_semantic_scholar(query, limit=20)
        
        all_papers = arxiv_papers + semantic_papers
        self.collector.save_papers(all_papers)
        print()
        
        # Step 2: T√≥m t·∫Øt
        print("B∆Ø·ªöC 2: T√ìM T·∫ÆT PAPERS")
        print("-" * 60)
        summarized_papers = self.summarizer.summarize_papers(all_papers)
        print()
        
        # Step 3: Ph√¢n t√≠ch xu h∆∞·ªõng
        print("B∆Ø·ªöC 3: PH√ÇN T√çCH XU H∆Ø·ªöNG")
        print("-" * 60)
        trends = self.trend_analyzer.identify_trends(summarized_papers)
        print()
        
        # Step 4: T·∫°o √Ω t∆∞·ªüng
        print("B∆Ø·ªöC 4: T·∫†O √ù T∆Ø·ªûNG NGHI√äN C·ª®U")
        print("-" * 60)
        ideas = self.idea_generator.generate_research_ideas(trends, summarized_papers)
        print()
        
        # Step 5: T·∫°o b√°o c√°o
        print("B∆Ø·ªöC 5: T·∫†O B√ÅO C√ÅO")
        print("-" * 60)
        report_path = self.report_generator.create_markdown_report(
            summarized_papers, trends, ideas
        )
        print()
        
        print("=" * 60)
        print(" HO√ÄN TH√ÄNH PIPELINE!")
        print("=" * 60)
        print(f"\n B√°o c√°o ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {report_path}")
        print(f" T·ªïng s·ªë papers: {len(all_papers)}")
        print(f" S·ªë √Ω t∆∞·ªüng ƒë·ªÅ xu·∫•t: {len(ideas)}")
        print()
        
        return {
            'papers': summarized_papers,
            'trends': trends,
            'ideas': ideas,
            'report_path': report_path
        }


def main():
    
    orchestrator = AgenticResearchOrchestrator()
    
    # C·∫•u h√¨nh
    query = "Agentic AI OR AI Agents OR Autonomous AI"
    days_back = 30
    max_papers = 50
    
    print(f" C·∫•u h√¨nh:")
    print(f"   - T·ª´ kh√≥a t√¨m ki·∫øm: {query}")
    print(f"   - Th·ªùi gian: {days_back} ng√†y g·∫ßn nh·∫•t")
    print(f"   - S·ªë l∆∞·ª£ng papers t·ªëi ƒëa: {max_papers}")
    print()
    
    # Ch·∫°y pipeline
    results = orchestrator.run_research_pipeline(query, days_back, max_papers)
    
    print(" H·ªá th·ªëng ƒë√£ ho√†n th√†nh!")
    print(" Ki·ªÉm tra th∆∞ m·ª•c 'reports' ƒë·ªÉ xem b√°o c√°o chi ti·∫øt")
    

if __name__ == "__main__":
    main()